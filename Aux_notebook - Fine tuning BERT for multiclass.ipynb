{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUXILIARY NOTEBOOK: Fine-Tuning BERT to multi-class prediction of document topic.\n",
    "----\n",
    "by Juan Equihua on April 10th, 2022.\n",
    "\n",
    "The purpose of this notebook is extending the 1-D labeling provided in the dataset to multiple classes. For this we will fine-tune the BERT model to achieve multi-class prediction, and then dump the model on disk from where we can deploy it via Docker.\n",
    "\n",
    "NOTE: This notebook saves the tuned BERT model in **/Fine_tuned_BERT/multiclassification_bert/**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFAutoModelForSequenceClassification, DefaultDataCollator\n",
    "\n",
    "from utils.utilities import load_data, get_predictions_from_bert_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specify input data:\n",
    "data_path = './Data/data.csv'\n",
    "data = load_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AUX Functions:\n",
    "## TODO: Comment the code.\n",
    "\n",
    "def get_labels(data):\n",
    "    return list(data.clusters_0.unique())\n",
    "\n",
    "def clean_text(text):\n",
    "    ''' Clean text. (Further cleaning can be done) '''\n",
    "    text = str(text).lower()\n",
    "    return text\n",
    "    \n",
    "def encode_label(label, labels):\n",
    "    return np.array(list(int(label == x) for x in labels))\n",
    "\n",
    "def encode_label_bkp(label, labels):\n",
    "    return np.array([int(label == x) for x in labels])[..., np.newaxis]\n",
    "\n",
    "def preprocess_data(data):\n",
    "    ''' Preprocess data for training '''\n",
    "    data['tag'] = [[str(x)] for x in data.clusters_0]\n",
    "    data['Clean_body'] = [clean_text(x) for x in data.body]\n",
    "    data['tag'] = [encode_label(x[0], labels) for x in data.tag]\n",
    "    return data[['Clean_body', 'tag' ]]\n",
    "\n",
    "def dict_row(row):\n",
    "    return {\"label\": row.tag, \"text\": row.Clean_body}\n",
    "        \n",
    "def create_examples(data):\n",
    "    ''' Create examples for all dataset '''\n",
    "    examples = []\n",
    "    for i in range(data.shape[0]):\n",
    "        examples.append(dict_row(data.iloc[i]))\n",
    "\n",
    "    return examples\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"Clean_body\"], padding=\"max_length\", truncation=True, max_length = 50)\n",
    "\n",
    "def make_predictions_for_string(string, tokenizer, model, labels):\n",
    "    tokenized_string_test = tokenizer.encode(string,\n",
    "                                            truncation=True,\n",
    "                                            padding=True,\n",
    "                                            return_tensors=\"tf\")\n",
    "    prediction = model(tokenized_string_test)[0]\n",
    "    prediction_probs = tf.nn.softmax(prediction,axis=1).numpy()\n",
    "    \n",
    "    sorted_prob_index = np.argsort(-prediction_probs)[0]\n",
    "    \n",
    "    return {\"Top 1\": labels[sorted_prob_index[0]], \"Top 2\": labels[sorted_prob_index[1]], \"Top 3\": labels[sorted_prob_index[2]]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Poor Pay', 'Cost of Living', 'Wage Growth', 'Rich People', 'Low Income Families', 'Public Sector Pay', 'Government Support', 'Mental Health', 'Leaseholding', 'State Pension', 'Pay Rises', 'Long Hours', 'Income Tax', 'Poor People', 'Council Tax', 'Small Businesses', 'Statutory Sick Pay', 'Social Care', 'House Prices', 'Job', 'Minimum Wage Increase', 'National Insurance', 'Gender Pay Gap']\n"
     ]
    }
   ],
   "source": [
    "## Create Labels: \n",
    "labels = get_labels(data)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean Data:\n",
    "dataset = preprocess_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load tokenizer for BERT: \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split data into train_test (hardcoded for now):\n",
    "dataset_train = dataset[:700]\n",
    "dataset_test = dataset[700:]\n",
    "\n",
    "## Convert pandas into Dataset: \n",
    "train_dataset = Dataset.from_pandas(dataset_train)\n",
    "test_dataset = Dataset.from_pandas(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a8a8c6146842e0904bc2820e63fcc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de404c0f15243b38341ba5232425855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Tokenize datasets: \n",
    "train_tokenized_datasets = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "## Define model:\n",
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create training, testing sets for Tensorflow: \n",
    "\n",
    "tf_train_dataset = train_tokenized_datasets.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=[\"tag\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "tf_validation_dataset = train_tokenized_datasets.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=[\"tag\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compile model:\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=tf.metrics.CategoricalAccuracy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "87/87 [==============================] - 454s 5s/step - loss: 2.9360 - categorical_accuracy: 0.1336 - val_loss: 2.8515 - val_categorical_accuracy: 0.1329\n",
      "Epoch 2/5\n",
      "87/87 [==============================] - 437s 5s/step - loss: 2.8690 - categorical_accuracy: 0.1279 - val_loss: 2.7762 - val_categorical_accuracy: 0.2071\n",
      "Epoch 3/5\n",
      "87/87 [==============================] - 403s 5s/step - loss: 2.8401 - categorical_accuracy: 0.1408 - val_loss: 2.7283 - val_categorical_accuracy: 0.2071\n",
      "Epoch 4/5\n",
      "87/87 [==============================] - 423s 5s/step - loss: 2.7149 - categorical_accuracy: 0.2040 - val_loss: 2.4548 - val_categorical_accuracy: 0.3057\n",
      "Epoch 5/5\n",
      "87/87 [==============================] - 407s 5s/step - loss: 2.3755 - categorical_accuracy: 0.3477 - val_loss: 1.8887 - val_categorical_accuracy: 0.4757\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd5c97f4e90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Model Training: (This might take a while).\n",
    "model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save model:\n",
    "model_path = './Fine_tuned_BERT/multiclassification_bert'\n",
    "model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Top 1': 'Cost of Living', 'Top 2': 'Mental Health', 'Top 3': 'Pay Rises'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Testing predictions: \n",
    "string_test = 'Economic impact of covid in real state'\n",
    "make_predictions_for_string(string_test, tokenizer, model, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing Loading Model and makinf predictions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ./Fine_tuned_BERT/multiclassification_bert were not used when initializing TFBertForSequenceClassification: ['dropout_37']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at ./Fine_tuned_BERT/multiclassification_bert.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "classifier = TFAutoModelForSequenceClassification.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Top 1': 'Cost of Living', 'Top 2': 'Mental Health', 'Top 3': 'Pay Rises'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_predictions_for_string(string_test, tokenizer, classifier, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test api requests after deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Top 1': 'Government Support',\n",
       " 'Top 2': 'Poor Pay',\n",
       " 'Top 3': 'Low Income Families'}"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## NOTE: get_predictions_from_bert_api function only works if the Flask API was already launched. \n",
    "get_predictions_from_bert_api('Covid impact in household income')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
